{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import utils\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function for displaying generated images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        x = x.detach().cpu()\n",
    "        if x.dim() == 3 and x.shape[0] == 1:\n",
    "            x = x.squeeze(0)\n",
    "        elif x.dim() == 4 and x.shape[1] == 1:\n",
    "            x = x[0, 0]\n",
    "        x = (x + 3) / 6.0\n",
    "        x = torch.clamp(x, 0, 1)\n",
    "        plt.imshow(x.numpy(), cmap=\"gray\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    else:\n",
    "        raise TypeError(\"Input must be a torch.Tensor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Layer (with time affine time encoding):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, time_encoding_dim = 16):\n",
    "        super(ConvolutionalBlock, self).__init__()\n",
    "\n",
    "        self.conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size = 4, stride = 2, padding = 1)\n",
    "\n",
    "        self.gn = torch.nn.GroupNorm(1, out_channels)\n",
    "        self.time_scale = torch.nn.Linear(time_encoding_dim, out_channels)\n",
    "        self.time_shift = torch.nn.Linear(time_encoding_dim, out_channels)\n",
    "\n",
    "        self.activation = torch.nn.SiLU()\n",
    "\n",
    "    def forward(self, x, time_encoding):\n",
    "        time_scale = self.time_scale(time_encoding).unsqueeze(-1).unsqueeze(-1)\n",
    "        time_shift = self.time_shift(time_encoding).unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        time_scale = self.activation(time_scale)\n",
    "        time_shift = self.activation(time_shift)\n",
    "\n",
    "\n",
    "        \n",
    "        x = self.conv(x)\n",
    "        \n",
    "        x = self.activation(x)\n",
    "        x = self.gn(x)\n",
    "\n",
    "        x = x * time_scale + time_shift # affine time encoding\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deconvolution (Transposed Convolution) with affine time encoding and skip connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalBlockTranspose(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, time_encoding_dim = 16, skip_connection_channels = 0):\n",
    "        super(ConvolutionalBlockTranspose, self).__init__()\n",
    "        self.conv = torch.nn.ConvTranspose2d(\n",
    "            in_channels + skip_connection_channels, \n",
    "            out_channels, \n",
    "            kernel_size = 4,\n",
    "            stride = 2, \n",
    "            padding = 1\n",
    "        )\n",
    "\n",
    "        self.gn = torch.nn.GroupNorm(1, out_channels)\n",
    "        self.time_scale = torch.nn.Linear(time_encoding_dim, out_channels)\n",
    "        self.time_shift = torch.nn.Linear(time_encoding_dim, out_channels)\n",
    "\n",
    "        self.activation = torch.nn.SiLU()\n",
    "\n",
    "    def forward(self, x, time_encoding, skip_connection):\n",
    "\n",
    "        time_scale = self.time_scale(time_encoding).unsqueeze(-1).unsqueeze(-1)\n",
    "        time_shift = self.time_shift(time_encoding).unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        time_scale = self.activation(time_scale)\n",
    "        time_shift = self.activation(time_shift)\n",
    "\n",
    "        x = torch.cat([x, skip_connection], dim = 1)\n",
    "\n",
    "        \n",
    "        x = self.conv(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.gn(x)\n",
    "\n",
    "        x = x * time_scale + time_shift # affine time encoding\n",
    "\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U-Net architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTUnet(torch.nn.Module):\n",
    "    def __init__(self, latent_dim, time_encoding_dim = 16):\n",
    "        super(MNISTUnet, self).__init__()\n",
    "        \n",
    "        self.encoder_conv = torch.nn.ModuleList([\n",
    "            ConvolutionalBlock(1, 32, time_encoding_dim),\n",
    "            ConvolutionalBlock(32, 64, time_encoding_dim),\n",
    "        ])\n",
    "\n",
    "        self.dense = torch.nn.Sequential(\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(64 * 7 * 7, latent_dim),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(latent_dim, 64 * 7 * 7),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Unflatten(1, (64, 7, 7))\n",
    "        )\n",
    "\n",
    "\n",
    "        self.decoder_conv = torch.nn.ModuleList([\n",
    "            ConvolutionalBlockTranspose(64, 32, time_encoding_dim, skip_connection_channels = 64),\n",
    "            ConvolutionalBlockTranspose(32, 1, time_encoding_dim, skip_connection_channels = 32)\n",
    "\n",
    "        ])\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.time_encoding_dim = time_encoding_dim\n",
    "\n",
    "        self.time_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(time_encoding_dim, time_encoding_dim * 4),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(time_encoding_dim * 4, time_encoding_dim),\n",
    "            torch.nn.SiLU(),\n",
    "        )\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "    def time_encoding(self, t):\n",
    "        sin = torch.cat([torch.sin(2 * math.pi * t * mul / self.steps).unsqueeze(1) for mul in range(1, self.time_encoding_dim // 2 + 1)], dim = 1)\n",
    "        cos = torch.cat([torch.cos(2 * math.pi * t * mul / self.steps).unsqueeze(1) for mul in range(1, self.time_encoding_dim // 2 + 1)], dim = 1)\n",
    "        \n",
    "        out = torch.cat([sin, cos], dim = 1)\n",
    "        return out\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        time_encoding = self.time_encoding(t)\n",
    "        time_encoding = self.time_mlp(time_encoding)\n",
    "\n",
    "        skip_connections = []\n",
    "        for idx, conv in enumerate(self.encoder_conv):\n",
    "            x = conv(x, time_encoding)\n",
    "            skip_connections.append(x.clone())\n",
    "\n",
    "        x = self.dense(x)\n",
    "\n",
    "        for idx, conv in enumerate(self.decoder_conv):\n",
    "            x = conv(x, time_encoding, skip_connections[-(idx + 1)])\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDiffusionModel(MNISTUnet):\n",
    "    def __init__(self, latent_dim = 2, time_encoding_dim = 2, beta_min = 0.001, beta_max = 0.02, steps = 1000):\n",
    "        super(MNISTDiffusionModel, self).__init__(latent_dim = latent_dim, time_encoding_dim = time_encoding_dim)\n",
    "\n",
    "        self.steps = steps\n",
    "\n",
    "        self.beta = torch.linspace(beta_min, beta_max, steps)\n",
    "        self.alpha = 1 - self.beta\n",
    "\n",
    "        self.alpha_hat = torch.cumprod(self.alpha, dim = 0)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "        self.loss_fn = torch.nn.MSELoss(reduction=\"mean\")\n",
    "\n",
    "        self.beta = self.beta.to(self.device)\n",
    "        self.alpha_hat = self.alpha_hat.to(self.device)\n",
    "        self.alpha = self.alpha.to(self.device)\n",
    "\n",
    "        self.reconstructing_noise = False\n",
    "\n",
    "\n",
    "    def train_step(self, x, t = None):\n",
    "        x = x.to(self.device)\n",
    "        if t is None:\n",
    "            t = torch.randint(0, len(self.alpha) - 1, (x.size(0),)).long()\n",
    "        noise = torch.randn_like(x).to(self.device)\n",
    "        t = t.to(self.device)\n",
    "        x_t = torch.sqrt(self.alpha_hat[t].reshape(-1, 1, 1, 1)) * x + torch.sqrt(1 - self.alpha_hat[t].reshape(-1, 1, 1, 1)) * noise\n",
    "        output = self(x_t, t)\n",
    "        if self.reconstructing_noise:\n",
    "            loss = self.loss_fn(output, noise)\n",
    "        else:\n",
    "            loss = self.loss_fn(output, x)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "    def sample(self, x = None, T = None):\n",
    "        if x is None:\n",
    "            x = torch.randn((1, 1, 28, 28)).to(self.device)\n",
    "        if T is None:\n",
    "            T = self.steps - 1\n",
    "        for t in tqdm(range(T - 1, 0, -1)):\n",
    "            t = torch.tensor([t]).to(self.device)\n",
    "            if self.reconstructing_noise:\n",
    "                recon_noise = self(x, t)\n",
    "            else:\n",
    "                recon_x = self(x, t)\n",
    "                recon_noise  = x - recon_x\n",
    "            mu = 1 / torch.sqrt(self.alpha[t]) * (x - (1 - self.alpha[t]) / torch.sqrt(1 - self.alpha_hat[t] + 1e-5) * recon_noise)\n",
    "            sigma = torch.sqrt(self.beta[t])\n",
    "            x = mu + sigma * torch.randn_like(x)\n",
    "        if self.reconstructing_noise:\n",
    "            recon_noise = self(x, torch.tensor([0]).to(self.device)).to(self.device)\n",
    "        else:\n",
    "            recon_x = self(x, torch.tensor([0]).to(self.device)).to(self.device)\n",
    "            recon_noise  = x - recon_x\n",
    "        x = 1 / torch.sqrt(self.alpha[0]) * (x - (1 - self.alpha[0]) / torch.sqrt(1 - self.alpha_hat[0] + 1e-5) * recon_noise)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "])\n",
    "    \n",
    "    \n",
    "train_dataset = datasets.MNIST(root='./data', train = True, download = True, transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [i for i, (_, label) in enumerate(train_dataset) if label == 7]\n",
    "train_dataset = Subset(train_dataset, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98/98 [00:02<00:00, 46.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 48.232021510601044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98/98 [00:02<00:00, 47.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 30.778204411268234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98/98 [00:02<00:00, 46.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 29.146579816937447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98/98 [00:02<00:00, 46.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 29.02911476790905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98/98 [00:02<00:00, 46.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 29.125365898013115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98/98 [00:02<00:00, 46.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 28.69750612974167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98/98 [00:02<00:00, 46.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 27.923026248812675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98/98 [00:02<00:00, 46.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 27.820910692214966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98/98 [00:02<00:00, 46.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 27.723757222294807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98/98 [00:02<00:00, 46.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 27.38690534234047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = 64, shuffle = True)\n",
    "model = MNISTDiffusionModel(latent_dim = 16, time_encoding_dim = 16, steps = 1000)\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    loss = 0\n",
    "    for x, _ in tqdm(train_loader):\n",
    "        loss += model.train_step(x)\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 998/998 [00:03<00:00, 300.95it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAFNhJREFUeJzt3Otv13f5x/ELCqUc2kFpgY7TOIzzYegGIRicTo1TEzd3Z3+BiTe84V3/C/0HTIwx8YYxi24e0DEdhzE2RDaYYxvjVAaDcWoLFFr6u3fld6+93on4yy+Px22e3w9rv+2L741d0yYmJiYCACJi+n/7LwDA/x1GAYBkFABIRgGAZBQASEYBgGQUAEhGAYA0Y6p/8Ac/+EH5xe/du1duuru7y01ExJEjR8rN8uXLy838+fPLTcv/H3j27NlyExGxdu3acjM2NlZuvvjii3KzYcOGchMRMTo6Wm6Gh4cfyXNu3bpVbh577LFyExHx1FNPlZvf/va35WbPnj3l5tChQ+Xm+eefLzcREa+++mq56erqKjd9fX3lplVnZ2e5mT69/m/6P//5z5O/bvlVAfh/yygAkIwCAMkoAJCMAgDJKACQjAIAySgAkIwCAMkoAJCMAgDJKACQpk1M8Vrb17/+9fKLtxy3u3PnTrmJiFi5cmW5uXr1arn5/PPPy83cuXPLzYoVK8pNRMTBgwfLzbZt28rN7Nmzy01vb2+5iYg4ffp0uZk1a1a5OX78eLnZu3dvuWl530VE3Lhxo6mr2rhxY7k5c+ZMuWk5mBkR8dlnn5WbzZs3l5uWg5kjIyPlJiLi0qVL5eb8+fPl5uOPP570z/ikAEAyCgAkowBAMgoAJKMAQDIKACSjAEAyCgAkowBAMgoAJKMAQDIKAKQZU/2DS5YsKb94yyGzZcuWlZuIiOvXr5ebmzdvlptdu3aVm5/97Gfl5uWXXy43ERGrVq0qN2+88Ua5Wb9+fbm5ePFiuYmIuH//frlpOUI4f/78cnPkyJFy0/K1i2j73p46darctPxcvP/+++Xma1/7WrmJaDse19HRUW5aDhf29PSUm4i2Q5v9/f1Nz5qMTwoAJKMAQDIKACSjAEAyCgAkowBAMgoAJKMAQDIKACSjAEAyCgAkowBAMgoApClfSd2yZUv5xS9fvlxuurq6yk1E25XBO3fulJs333yz3DzxxBPl5vjx4+UmIqKzs7PczJgx5bdBGhgYKDfDw8PlJiJi3rx55ebDDz8sN9u2bSs3ExMT5ebAgQPlJiJix44d5abl8uvChQvLzQsvvFBuWr5HEW2XSMfHx8vNzJkzy817771XbiIiNmzYUG4GBwebnjUZnxQASEYBgGQUAEhGAYBkFABIRgGAZBQASEYBgGQUAEhGAYBkFABIRgGANG1iihe9nn766fKLj4yMlJvp09t2avv27eXm3r175ablKNmxY8fKTcuxvoiIjo6OcjM0NFRuWo6FtRz9iog4ePBgufnyl79cbo4ePVpu+vv7y03Lgb+IiN7e3nJz+/btctNylPL69evl5ty5c+UmImLNmjXl5uHDh4+k+de//lVuItrery2/I15//fVJ/4xPCgAkowBAMgoAJKMAQDIKACSjAEAyCgAkowBAMgoAJKMAQDIKACSjAECaMdU/uG7duvKLf/TRR+XmscceKzcRESdOnCg3mzZtKjctx+3u3r1bbs6fP19uIiKefPLJcrN06dJy03Jobc6cOeUmIuLq1avlpuX71PL3a/najY2NlZuItp+nRYsWNT2rqru7u9ysXLmy6VmDg4PlZuPGjeXmwoUL5Wbv3r3lJqLtwOTZs2ebnjUZnxQASEYBgGQUAEhGAYBkFABIRgGAZBQASEYBgGQUAEhGAYBkFABIRgGANOWDeKdPny6/+M6dO8vNK6+8Um4iIh4+fFhuhoaGys3w8HC5uXnzZrlpOeAV0XbUbd68eeXm2rVr5abloFtExAsvvFBujh49Wm4WLlxYbkZGRspNV1dXuYmImDt3brk5fPhwufnSl75Ubvr6+srN6OhouYmI6OjoKDdr1qwpN/fv3y83ly5dKjcREcuXLy83q1evbnrWZHxSACAZBQCSUQAgGQUAklEAIBkFAJJRACAZBQCSUQAgGQUAklEAIBkFANKUD+K1HGy6cOFCuVm3bl25iYi4ceNGuWk5TNbT01NuWg5rtRypi4h46aWXys1TTz1Vbh48eFBuJiYmyk1ExOXLl8vN+Ph4ublz5065aTlS1/IeiojYvXt3udmzZ0+5+elPf1pubt26VW5aDyQeOnSo3Fy5cqXcDA4OlpvFixeXm4iIY8eOlZvvfOc7Tc+ajE8KACSjAEAyCgAkowBAMgoAJKMAQDIKACSjAEAyCgAkowBAMgoAJKMAQDIKAKQpX0ltuXC5bNmycnPixIlyExHR399fbmbMmPJ/fjp+/Hi5eeaZZ8rN7du3y01ExK5du8pNX19fuZk+vf7viZavXUTbddCWr0NnZ2e5+dvf/lZu1q9fX24iIubPn19uWq/tVrW8h1qaiLaLzb/5zW+anlW1evXqpq7lIuupU6eanjUZnxQASEYBgGQUAEhGAYBkFABIRgGAZBQASEYBgGQUAEhGAYBkFABIRgGANOWLcIsWLSq/+LVr18rN7Nmzy01ExNjYWLkZHh4uN88991y5uX//frl5+eWXy01ERG9vb7lpOQzY4vHHH2/qrl69Wm4GBgaanlXV8n5o1fJ+HR8fLzfvvPNOuVmxYkW5af0ePXjwoNwcPHiw6VlVrUfqrl+/Xm7+Uz+3PikAkIwCAMkoAJCMAgDJKACQjAIAySgAkIwCAMkoAJCMAgDJKACQjAIAacoXlU6fPl1+8dHR0XKzdevWchMRcenSpXJz9uzZcvPw4cNys2fPnnIzd+7cchMR0dnZWW5aDq21HCW7e/duuYmImDlzZrnZt29fuVm+fHm5mTVrVrlp+bmIiFi1alW5afk+LVmypNy0HrJs8dFHH5WblqOKLV+7bdu2lZuIiP7+/nLz2muvNT1rMj4pAJCMAgDJKACQjAIAySgAkIwCAMkoAJCMAgDJKACQjAIAySgAkIwCAGnKB/Fu3rxZfvHFixeXm0OHDpWbiIi9e/eWmzt37pSbBQsWlJuWo2ktR+AiIm7cuFFuWg7i7d+/v9z84Q9/KDcREfPnzy83H3/8cbnp7u4uN729veXmxz/+cbmJaHsftRx9fOKJJ8rNxYsXy82vf/3rchPR9r29fv16uenr6ys3t27dKjcREQcOHCg33/rWt5qeNRmfFABIRgGAZBQASEYBgGQUAEhGAYBkFABIRgGAZBQASEYBgGQUAEhGAYBkFABIU76S+vTTT5df/PDhw+Wms7Oz3EREfPDBB+Vm2bJl5WZwcLDcHDlypNyMjIyUm4iIN998s9z885//LDfnz58vN6Ojo+UmIqKrq6vcPPbYY+Wmo6Oj3CxduvSRNBERn3zySblpeY+3+PTTT8tNy2XQiLaf9Zav3a5du8rNP/7xj3ITEbF58+Zyc/r06aZnTcYnBQCSUQAgGQUAklEAIBkFAJJRACAZBQCSUQAgGQUAklEAIBkFAJJRACBN+SDe1atXyy++YMGCcvPgwYNyExExMDBQbmbPnl1ubty4UW7mzZtXbn75y1+Wm4iIZ599ttycO3eu3OzcubPcnDhxotxERMyfP7/cDA0NlZuW99BXv/rVcjM8PFxuIiLWrFnT1FWNjY2Vm5YjdTt27Cg3EW1/v5afwZMnT5abVi2HQGfOnPkf+Jv4pADA/2IUAEhGAYBkFABIRgGAZBQASEYBgGQUAEhGAYBkFABIRgGAZBQASFM+iHfmzJnyi2/YsKHc3L17t9xERLz77rvlpre3t9w8fPiw3Bw9erTctBxai4i4d+9euZk+vf5vg7fffrvcbN++vdxEROzfv7/c9PT0lJsVK1aUm927d5ebluNsERETExPl5pNPPik3LQfn/vSnP5WbmzdvlptW9+/fLze3bt0qNy1HFSPaDm2Oj483PWsyPikAkIwCAMkoAJCMAgDJKACQjAIAySgAkIwCAMkoAJCMAgDJKACQjAIAacoH8Z588snyi7cct7t27Vq5iYh48cUXy82rr75abloOrW3evLncnDp1qtxEREybNq3ctByCa/k+nT17ttxERKxdu7bctBx1e+mll8pN63G7FqOjo+Wm5Wfwgw8+eCTPuXTpUrmJaHu/7ty5s9y89dZb5WbRokXlJiJiZGSk3MyYMeVf3yU+KQCQjAIAySgAkIwCAMkoAJCMAgDJKACQjAIAySgAkIwCAMkoAJCMAgBpyheVWg42dXV1lZvWI09nzpwpN9On1zdx2bJl5WZ4eLjcbN26tdxEtH2fZs2aVW4WLlxYbi5cuFBuIiJ27NhRbr7//e+XmwcPHpSbW7dulZuWn4uIiKGhoXJz4MCBcvP73/++3LR87fr6+spNRMTs2bPLzeDgYLlp+f1w+fLlchMRMTAwUG42btzY9KzJ+KQAQDIKACSjAEAyCgAkowBAMgoAJKMAQDIKACSjAEAyCgAkowBAMgoAJKMAQJrySdJNmzaVX/yNN94oNx0dHeUmIuL69evlpuXK4NjYWLl5/PHHy83t27fLTUTbldkbN26Umy+++KLc9Pf3l5uIiJUrV5ab7du3l5uWq5gt74eWi74REfv27Ss3//73v8tNT09Pufnwww/LzZo1a8pNRNsl0pbrwS2/H27evFluIiImJibKzXvvvdf0rMn4pABAMgoAJKMAQDIKACSjAEAyCgAkowBAMgoAJKMAQDIKACSjAEAyCgCkKV9Pazl4tWPHjnLz0UcflZuIiLt375abkydPlpvVq1eXmxUrVpSbd955p9xERMyaNavctHztlixZUm6WL19ebiIifvjDH5abgYGBcjM6OlpuWo6ztRy2i4g4duxYuRkcHCw3vb295ebb3/52ublw4UK5iWg7ztlyuHB4eLjcDA0NlZuItmOMLe+9qfBJAYBkFABIRgGAZBQASEYBgGQUAEhGAYBkFABIRgGAZBQASEYBgGQUAEhTPojXcujpqaeeKjcffPBBuYloO3j1ve99r9z89a9/LTct/03nzp0rNxER/f395aa7u7vcHD16tNz86Ec/KjcREXPmzCk34+Pj5WZkZKTc/P3vfy83r7/+ermJaDtKOX/+/HIzY8aUfy2klqOKLT+zEREnTpwoNy3f25YDjjdv3iw3EW3HDrds2dL0rMn4pABAMgoAJKMAQDIKACSjAEAyCgAkowBAMgoAJKMAQDIKACSjAEAyCgCkaRMTExNT+YPf+MY3yi/e2dlZbubOnVtuIiLOnj1bbrq6usrN9On1HR0dHS03LYfMIiK+8pWvlJtr166Vm+9+97vlpuVAYkTbkb8Wf/zjH8vNL37xi3LTcjwuImLZsmXl5ne/+90jec7AwEC5aT0eNzw8/EiarVu3PpLnRDy6I4T79++f9M/4pABAMgoAJKMAQDIKACSjAEAyCgAkowBAMgoAJKMAQDIKACSjAEAyCgCkKV9hevjwYfnF79y5U25adXd3l5sLFy6Um7Vr15ab69evl5ve3t5yExFx6dKlcvPiiy+Wm927d5ebefPmlZtW4+Pj5ebnP/95uTlx4kS52bx5c7mJiDh37ly52bZtW7lp+VmfNm1auWn5mY1oO9jXcmDy3r175aanp6fcRLQd0rt//37TsybjkwIAySgAkIwCAMkoAJCMAgDJKACQjAIAySgAkIwCAMkoAJCMAgDJKACQjAIAacpXUvv7+8svfvr06XLTeh30wYMH5WbDhg3l5tSpU+Vm79695ebGjRvlJqLtius3v/nNpmdVDQ0NNXVdXV3l5pVXXik3CxcuLDebNm0qN6Ojo+Umou3K7JUrV8rN7Nmzy82MGVP+VZLGxsbKTUTExx9/XG5a3nstv/NWr15dbiLaLkpfvHix6VmT8UkBgGQUAEhGAYBkFABIRgGAZBQASEYBgGQUAEhGAYBkFABIRgGAZBQASFO+YvX222+XX7zlkFlLExHR3d1dbloO9j333HPl5sSJE+Vm8eLF5Sai7djaxMREuZk2bVq56ezsLDcREcePHy83b731VrlpOUrWcgiu5b0aEXHu3LlyM2fOnHIzODhYbloOUi5YsKDcRLR9zVt+ntasWVNuWn7WI9qOMT799NNNz5qMTwoAJKMAQDIKACSjAEAyCgAkowBAMgoAJKMAQDIKACSjAEAyCgAkowBAmvJlqeeff7784p9++mm56ejoKDcRbQe5tmzZUm7ee++9crN69epy09fXV24iIn7yk5+Um7GxsXLz8OHDcnP06NFyExGxb9++cnPw4MFys3LlynJz5cqVcnP79u1yE9F2NK3lwGTL+6Hl8N67775bbiIiNmzYUG5afp5afn/19vaWm4iIzz///JE9azI+KQCQjAIAySgAkIwCAMkoAJCMAgDJKACQjAIAySgAkIwCAMkoAJCMAgBpygfxzpw5U37xBQsWlJuW50RErFu3rtycPn263LQcJZs7d2652bRpU7mJiDh37ly5mTlzZrlpOfJ3586dchMRMTIyUm5OnTpVbnp6esrN4sWLy03LkbqIiEuXLpWbadOmlZtt27aVm5b33ZIlS8pNRMTJkyfLzTPPPFNu7t+/X276+/vLTUTbwdHWg4KT8UkBgGQUAEhGAYBkFABIRgGAZBQASEYBgGQUAEhGAYBkFABIRgGAZBQASEYBgDTlK6nj4+PlFx8eHi43LRc7IyIGBwfLTUdHR7lZsWJFuWm5xtrytWvtZs2aVW5mzJjyWyf96le/KjcREZ9//nm5abk6OX16/d9Ihw8fLjct1zcjIjZu3FhuWt4PDx48KDctP7etV3O3bt1ablquuHZ3d5ebzz77rNxERLz22mvlpq+vr+lZk/FJAYBkFABIRgGAZBQASEYBgGQUAEhGAYBkFABIRgGAZBQASEYBgGQUAEhTvmp279698ovfvn273LQcu4poO3g1Ojpabvbt21duli5dWm6GhobKTUTEX/7yl3Jz5cqVcvPFF1+Um4GBgXITEbFq1apy03KobteuXeWm5UBiy/HGiIiRkZFy86jee5cvXy43e/fuLTcREdeuXSs3PT095eb9998vN88++2y5iWh7v65fv77pWZPxSQGAZBQASEYBgGQUAEhGAYBkFABIRgGAZBQASEYBgGQUAEhGAYBkFABI0yYmJib+238JAP5v8EkBgGQUAEhGAYBkFABIRgGAZBQASEYBgGQUAEhGAYD0P3GvsyDY5xaGAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    recon = model.sample()\n",
    "    utils.display(recon)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
